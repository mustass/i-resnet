{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iresnet.datasets import get_cifar10_data\n",
    "from iresnet.models import ResNet18\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from jaxtyping import Array, Float, Int, PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "STEPS = 1200\n",
    "PRINT_EVERY = 30\n",
    "SEED = 5678\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trn, tst = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 32)\n",
      "(64,)\n",
      "[3 3 2 3 3 9 5 1 3 2 5 0 0 5 0 2 5 3 5 6 8 2 1 5 1 2 8 1 9 5 1 8 3 6 1 3 0\n",
      " 1 8 4 5 7 0 5 6 4 1 5 5 1 4 6 8 7 6 1 2 3 1 0 7 2 2 6]\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trn))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # batch_size x3x32x32\n",
    "print(dummy_y.shape)  # batch_size\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "model = ResNet18(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[64,3,3,3],\n",
      "      bias=None,\n",
      "      in_channels=3,\n",
      "      out_channels=64,\n",
      "      kernel_size=(3, 3),\n",
      "      stride=(1, 1),\n",
      "      padding=((1, 1), (1, 1)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=False\n",
      "    ),\n",
      "    BatchNorm(\n",
      "      weight=f32[64],\n",
      "      bias=f32[64],\n",
      "      first_time_index=StateIndex(inference=False),\n",
      "      state_index=StateIndex(inference=False),\n",
      "      axis_name='batch',\n",
      "      inference=False,\n",
      "      input_size=64,\n",
      "      eps=1e-05,\n",
      "      channelwise_affine=True,\n",
      "      momentum=0.99\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,64,1,1],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=128,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,128,1,1],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=256,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,256,1,1],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=512,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    AvgPool2d(\n",
      "      init=0,\n",
      "      operation=<function add>,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    <wrapped function ravel>,\n",
      "    Linear(\n",
      "      weight=f32[10,512],\n",
      "      bias=f32[10],\n",
      "      in_features=512,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function log_softmax>\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Int, Float\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def loss(\n",
    "    model: ResNet18, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_y = jax.vmap(model, axis_name=\"batch\")(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of layer 0 is:  (64, 32, 32)\n",
      "Output Shape of layer 1 is:  (64, 32, 32)\n",
      "Output Shape of layer 2 is:  (64, 32, 32)\n",
      "Output Shape of layer 3 is:  (64, 32, 32)\n",
      "Output Shape of layer 4 is:  (64, 32, 32)\n",
      "Output Shape of layer 5 is:  (128, 16, 16)\n",
      "Output Shape of layer 6 is:  (128, 16, 16)\n",
      "Output Shape of layer 7 is:  (256, 8, 8)\n",
      "Output Shape of layer 8 is:  (256, 8, 8)\n",
      "Output Shape of layer 9 is:  (512, 4, 4)\n",
      "Output Shape of layer 10 is:  (512, 4, 4)\n",
      "Output Shape of layer 11 is:  (512, 1, 1)\n",
      "Output Shape of layer 12 is:  (512,)\n",
      "Output Shape of layer 13 is:  (10,)\n",
      "Output Shape of layer 14 is:  (10,)\n",
      "()\n",
      "Output Shape of layer 0 is:  (64, 32, 32)\n",
      "Output Shape of layer 1 is:  (64, 32, 32)\n",
      "Output Shape of layer 2 is:  (64, 32, 32)\n",
      "Output Shape of layer 3 is:  (64, 32, 32)\n",
      "Output Shape of layer 4 is:  (64, 32, 32)\n",
      "Output Shape of layer 5 is:  (128, 16, 16)\n",
      "Output Shape of layer 6 is:  (128, 16, 16)\n",
      "Output Shape of layer 7 is:  (256, 8, 8)\n",
      "Output Shape of layer 8 is:  (256, 8, 8)\n",
      "Output Shape of layer 9 is:  (512, 4, 4)\n",
      "Output Shape of layer 10 is:  (512, 4, 4)\n",
      "Output Shape of layer 11 is:  (512, 1, 1)\n",
      "Output Shape of layer 12 is:  (512,)\n",
      "Output Shape of layer 13 is:  (10,)\n",
      "Output Shape of layer 14 is:  (10,)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example loss\n",
    "loss_value = loss(model, dummy_x, dummy_y)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "# Example inference\n",
    "output = jax.vmap(model,axis_name=\"batch\")(dummy_x)\n",
    "print(output.shape)  # batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.9125946, -2.2175357, -2.737707 , -1.9967215, -2.8336525,\n",
       "        -2.352201 , -1.7477696, -2.2363877, -2.7961183, -1.9641101],\n",
       "       [-3.1072884, -2.4489298, -2.8389645, -2.0237842, -2.628903 ,\n",
       "        -2.5898547, -1.8109677, -1.8311937, -2.636527 , -1.9970496],\n",
       "       [-3.2473755, -2.1117609, -2.8919764, -2.3016863, -2.9551249,\n",
       "        -2.5209174, -1.6660374, -2.2128782, -2.4742818, -1.7750113],\n",
       "       [-3.1889958, -2.0593648, -3.19628  , -2.2887795, -2.629171 ,\n",
       "        -2.446034 , -1.7082007, -1.9799814, -2.5837007, -1.9994087],\n",
       "       [-2.903036 , -1.9820261, -2.9480648, -2.1370368, -2.778913 ,\n",
       "        -2.6690097, -2.0153184, -1.9490082, -2.4834828, -1.921846 ],\n",
       "       [-2.8478565, -2.1506553, -2.743957 , -2.0174518, -3.0017042,\n",
       "        -2.6968443, -1.7695409, -2.0603173, -2.6106029, -1.9669756],\n",
       "       [-3.0763302, -2.34106  , -2.9499195, -2.191332 , -2.5909436,\n",
       "        -2.5179925, -1.7239666, -1.842611 , -2.5679805, -2.0833452],\n",
       "       [-3.3212318, -2.0852141, -2.7887435, -2.4093804, -2.8550935,\n",
       "        -2.3658552, -1.7527139, -2.0536768, -2.6036656, -1.8247674],\n",
       "       [-3.2554123, -2.0902956, -3.0893166, -2.3892715, -2.723789 ,\n",
       "        -2.6240697, -1.8400683, -2.0831134, -2.2629373, -1.7428021],\n",
       "       [-2.7643304, -2.1964734, -2.6548421, -1.9133723, -2.8312092,\n",
       "        -2.437169 , -1.9505382, -2.2558188, -2.809054 , -1.8688657],\n",
       "       [-3.2379575, -2.2305622, -3.0661478, -2.268301 , -2.535955 ,\n",
       "        -2.5935197, -1.7513776, -1.910563 , -2.6458492, -1.8529088],\n",
       "       [-3.0215316, -2.1914382, -3.0723825, -2.1719923, -3.003582 ,\n",
       "        -2.7316113, -1.6461034, -1.9672493, -2.6364746, -1.830838 ],\n",
       "       [-3.0721428, -1.8842866, -3.031187 , -1.9925597, -2.9510205,\n",
       "        -2.7358422, -1.8823345, -1.9681008, -2.6551733, -1.9821854],\n",
       "       [-2.9862082, -2.3036747, -2.6550283, -2.1384618, -2.7214723,\n",
       "        -2.4790087, -1.913713 , -2.1462722, -2.3485608, -1.884295 ],\n",
       "       [-3.458681 , -1.6920968, -3.3237138, -2.5079522, -3.0830839,\n",
       "        -2.1006527, -1.9392092, -2.204108 , -2.446416 , -1.8453003],\n",
       "       [-2.8926754, -2.0369825, -2.8367176, -2.152153 , -2.8795142,\n",
       "        -2.6485558, -1.9877331, -1.9498816, -2.8812897, -1.7314652],\n",
       "       [-2.9863596, -2.2149017, -2.6837354, -2.0796213, -2.679149 ,\n",
       "        -2.4628384, -1.8506687, -2.0672739, -2.6411247, -1.9781691],\n",
       "       [-3.1434999, -2.1036696, -3.0961778, -2.2255664, -2.6558409,\n",
       "        -2.582466 , -1.7565851, -1.9019806, -2.603864 , -1.9667403],\n",
       "       [-3.1690178, -2.163584 , -2.9601684, -2.1368558, -2.856461 ,\n",
       "        -2.5449448, -1.8400598, -2.0228715, -2.447931 , -1.8344421],\n",
       "       [-3.019514 , -2.2363973, -2.7580395, -2.0986004, -2.7760916,\n",
       "        -2.6719742, -1.7393417, -2.0052078, -2.5374608, -1.9837229],\n",
       "       [-3.3826666, -1.8889445, -3.4711337, -2.3978577, -2.8139057,\n",
       "        -2.5303748, -1.7256365, -1.929475 , -2.4176207, -1.960411 ],\n",
       "       [-3.0072126, -2.2156272, -2.8578198, -2.2671099, -2.6166866,\n",
       "        -2.525027 , -1.8294401, -2.0775888, -2.532892 , -1.8188277],\n",
       "       [-3.6387882, -1.7585623, -3.6460693, -2.1614423, -2.6435943,\n",
       "        -2.7691464, -1.8756995, -1.7557687, -2.4787085, -2.150064 ],\n",
       "       [-3.0891447, -2.1306224, -3.0838928, -2.2205966, -2.5909781,\n",
       "        -2.5138612, -1.9985086, -2.0377812, -2.3804524, -1.7891301],\n",
       "       [-3.5593238, -1.991323 , -3.5681818, -2.2876914, -2.8528109,\n",
       "        -2.6633663, -1.6722996, -2.1769288, -2.352377 , -1.7060704],\n",
       "       [-2.919053 , -2.3380184, -2.7942872, -2.1221619, -2.7824926,\n",
       "        -2.509417 , -1.8011111, -2.0039072, -2.410315 , -1.9976327],\n",
       "       [-3.3877892, -2.0295393, -3.084067 , -2.449054 , -2.8631954,\n",
       "        -2.5334249, -1.8191539, -1.8142228, -2.561527 , -1.8085856],\n",
       "       [-3.0513816, -2.2301772, -2.9819956, -2.2559237, -2.7769918,\n",
       "        -2.3879247, -1.8762783, -1.9851599, -2.4633522, -1.8324785],\n",
       "       [-4.0142097, -1.952595 , -4.313337 , -3.0305367, -3.1964488,\n",
       "        -2.0879056, -1.6591626, -1.9864354, -2.1388898, -1.7823765],\n",
       "       [-3.1956778, -2.2367268, -2.9596176, -2.5230718, -2.9000015,\n",
       "        -2.609459 , -1.4768283, -2.1800828, -2.2813268, -1.9101167],\n",
       "       [-2.9743142, -2.0321007, -3.329763 , -2.3035908, -2.68646  ,\n",
       "        -2.4557178, -1.7640394, -1.9518744, -2.59961  , -1.9620371],\n",
       "       [-3.7299335, -2.3292725, -3.7814505, -2.8974352, -2.7969465,\n",
       "        -1.960787 , -1.7006819, -1.9488837, -2.2639332, -1.7719071],\n",
       "       [-2.84113  , -2.182397 , -2.8777614, -1.8984939, -2.9078321,\n",
       "        -2.5969296, -1.8975989, -1.9311497, -2.6154351, -2.0740702],\n",
       "       [-3.2904196, -2.07821  , -2.980257 , -2.4053104, -2.678197 ,\n",
       "        -2.6327717, -1.6770084, -2.1023178, -2.6918895, -1.7194395],\n",
       "       [-3.3545828, -1.9416676, -3.4783409, -2.857321 , -2.970543 ,\n",
       "        -2.384201 , -1.6055756, -2.1954331, -2.441018 , -1.6568638],\n",
       "       [-3.3031805, -2.1230714, -3.0132065, -2.5465136, -2.9954863,\n",
       "        -2.5176222, -1.6796947, -2.0464287, -2.2818956, -1.7855678],\n",
       "       [-2.8691049, -2.2772262, -2.6641283, -1.9538765, -2.6777315,\n",
       "        -2.579084 , -1.9355853, -2.3832521, -2.547213 , -1.7728597],\n",
       "       [-3.015895 , -2.2437596, -2.9584098, -2.1166546, -2.8031256,\n",
       "        -2.4800334, -1.6790102, -2.170202 , -2.329018 , -2.0391548],\n",
       "       [-2.9161074, -2.1513052, -2.7678804, -2.0807664, -2.721744 ,\n",
       "        -2.4184008, -1.8081841, -2.1120672, -2.6645856, -2.0211513],\n",
       "       [-2.8121614, -2.138111 , -2.7217023, -1.8664079, -2.7202692,\n",
       "        -2.4472852, -2.0130427, -2.1789174, -2.7342324, -1.9838974],\n",
       "       [-2.8934221, -2.1816163, -2.7896664, -2.0657847, -2.9606037,\n",
       "        -2.2717266, -2.087449 , -2.0174842, -2.519311 , -1.8893852],\n",
       "       [-2.918305 , -2.2266426, -2.7861404, -1.9640265, -2.7407832,\n",
       "        -2.309309 , -1.83032  , -2.0436945, -2.6642163, -2.1828613],\n",
       "       [-3.074841 , -1.9831401, -3.126154 , -2.2837377, -2.9556718,\n",
       "        -2.2608247, -1.8212478, -2.1975822, -2.4011874, -1.8933654],\n",
       "       [-3.2348063, -2.138659 , -2.7231734, -1.9916534, -2.6887994,\n",
       "        -2.9229813, -1.8378053, -1.8489287, -2.6185403, -2.044331 ],\n",
       "       [-2.9692507, -2.0158005, -2.6557527, -2.0128486, -2.810063 ,\n",
       "        -2.4001875, -1.9424932, -2.2707982, -2.5941095, -1.9704033],\n",
       "       [-2.858075 , -2.2501903, -2.6505687, -2.1600065, -2.792451 ,\n",
       "        -2.4770365, -1.924839 , -2.0608573, -2.516004 , -1.8837569],\n",
       "       [-3.3492825, -2.2462542, -3.1667023, -2.563321 , -2.9388313,\n",
       "        -2.2149758, -1.6528006, -2.171524 , -2.0848637, -1.9106741],\n",
       "       [-3.2641358, -2.0447984, -2.8346014, -2.204005 , -2.5169346,\n",
       "        -2.6548953, -1.9049641, -2.048333 , -2.6229143, -1.8209001],\n",
       "       [-2.904684 , -2.307237 , -2.6830237, -1.9679236, -2.6126118,\n",
       "        -2.3039412, -1.9945669, -2.088882 , -2.7266893, -1.9728919],\n",
       "       [-3.2498584, -1.8853035, -3.3079326, -2.3318076, -2.6156592,\n",
       "        -2.618765 , -1.851263 , -2.1461415, -2.457041 , -1.7716525],\n",
       "       [-2.9300585, -2.3501945, -2.5644464, -1.8069172, -2.849088 ,\n",
       "        -2.5891123, -1.9580469, -2.1179862, -2.7372372, -1.8905138],\n",
       "       [-2.8860617, -2.0526795, -2.7388015, -2.1595006, -2.9574895,\n",
       "        -2.632995 , -1.9420586, -2.0096269, -2.6077147, -1.8274598],\n",
       "       [-3.2774544, -1.8898544, -3.0430346, -2.4407444, -2.8372831,\n",
       "        -2.78693  , -1.7297767, -1.8757946, -2.5779767, -1.8990101],\n",
       "       [-3.046168 , -2.0799825, -2.9738128, -2.053297 , -2.912836 ,\n",
       "        -2.983398 , -1.9059389, -1.893051 , -2.4682853, -1.8379643],\n",
       "       [-2.9536457, -2.210462 , -2.5977097, -2.1092505, -2.984857 ,\n",
       "        -2.609045 , -1.8873703, -2.2531056, -2.5328987, -1.7020679],\n",
       "       [-3.1128523, -2.2227259, -2.9578342, -2.164786 , -2.7482216,\n",
       "        -2.5383978, -1.718327 , -2.0593889, -2.533982 , -1.8889391],\n",
       "       [-3.3102489, -2.239879 , -2.8648686, -2.1616862, -2.9791903,\n",
       "        -2.6300623, -1.8732429, -1.9420971, -2.516522 , -1.6916003],\n",
       "       [-3.5853653, -2.1118424, -3.034237 , -2.3669143, -2.6062772,\n",
       "        -2.3087735, -1.6240469, -1.9735165, -2.589611 , -2.078883 ],\n",
       "       [-2.9280734, -2.1511288, -2.91837  , -2.1716554, -2.834004 ,\n",
       "        -2.3141134, -1.8878796, -2.1352508, -2.6031654, -1.8272225],\n",
       "       [-3.5041118, -2.0679495, -3.200553 , -2.3980787, -2.8175411,\n",
       "        -2.6869254, -1.6722323, -1.9619069, -2.4816077, -1.760371 ],\n",
       "       [-3.4753437, -2.1670928, -3.1464014, -2.6850104, -2.6259515,\n",
       "        -2.3942122, -1.9974443, -1.8007047, -2.3588653, -1.6911638],\n",
       "       [-3.0753767, -1.899683 , -3.171804 , -2.5614734, -2.888186 ,\n",
       "        -2.6507883, -1.7331553, -1.9832366, -2.4854186, -1.8252301],\n",
       "       [-2.8476114, -2.2414298, -2.6682417, -1.8839169, -2.633822 ,\n",
       "        -2.5282984, -2.005358 , -2.1300888, -2.795906 , -1.9088774],\n",
       "       [-3.1138115, -2.249743 , -2.7438316, -2.2506158, -2.885469 ,\n",
       "        -2.4689457, -1.9945573, -1.9631317, -2.7113411, -1.6243358]],      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: ResNet18, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = jax.vmap(model,axis_name=\"batch\")(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def evaluate(model: ResNet18, testloader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: ResNet18,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    ") -> ResNet18:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: ResNet18,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 3 32 32\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, testloader)\n",
    "            print(\n",
    "                f\"{step=}, train_loss={train_loss.item()}, \"\n",
    "                f\"test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\"\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, trn, tst, optim, STEPS, PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20 - 2) // 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
