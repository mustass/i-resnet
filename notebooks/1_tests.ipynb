{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iresnet.datasets import get_cifar10_data\n",
    "from iresnet.models import ResNet18\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from jaxtyping import Array, Float, Int, PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "STEPS = 1200\n",
    "PRINT_EVERY = 30\n",
    "SEED = 5678\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trn, tst = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 32)\n",
      "(64,)\n",
      "[8 5 6 1 8 3 8 4 8 7 2 0 4 3 1 7 9 5 4 7 3 5 0 6 0 1 8 5 3 0 1 5 7 6 6 3 4\n",
      " 2 2 9 7 3 8 6 0 3 7 8 2 6 7 1 0 9 1 6 3 0 7 6 4 1 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trn))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # batch_size x3x32x32\n",
    "print(dummy_y.shape)  # batch_size\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "model = ResNet18(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[64,3,3,3],\n",
      "      bias=None,\n",
      "      in_channels=3,\n",
      "      out_channels=64,\n",
      "      kernel_size=(3, 3),\n",
      "      stride=(1, 1),\n",
      "      padding=((1, 1), (1, 1)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=False\n",
      "    ),\n",
      "    BatchNorm(\n",
      "      weight=f32[64],\n",
      "      bias=f32[64],\n",
      "      first_time_index=StateIndex(inference=False),\n",
      "      state_index=StateIndex(inference=False),\n",
      "      axis_name='batch',\n",
      "      inference=False,\n",
      "      input_size=64,\n",
      "      eps=1e-05,\n",
      "      channelwise_affine=True,\n",
      "      momentum=0.99\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[64,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=64,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[64],\n",
      "          bias=f32[64],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=64,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,64,3,3],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,64,1,1],\n",
      "          bias=None,\n",
      "          in_channels=64,\n",
      "          out_channels=128,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[128,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=128,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[128],\n",
      "          bias=f32[128],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=128,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,128,3,3],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,128,1,1],\n",
      "          bias=None,\n",
      "          in_channels=128,\n",
      "          out_channels=256,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[256,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=256,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[256],\n",
      "          bias=f32[256],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=256,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,256,3,3],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(2, 2),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,256,1,1],\n",
      "          bias=None,\n",
      "          in_channels=256,\n",
      "          out_channels=512,\n",
      "          kernel_size=(1, 1),\n",
      "          stride=(2, 2),\n",
      "          padding=((0, 0), (0, 0)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    BasicBlock(\n",
      "      layers=[\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        ),\n",
      "        <wrapped function relu>,\n",
      "        Conv2d(\n",
      "          num_spatial_dims=2,\n",
      "          weight=f32[512,512,3,3],\n",
      "          bias=None,\n",
      "          in_channels=512,\n",
      "          out_channels=512,\n",
      "          kernel_size=(3, 3),\n",
      "          stride=(1, 1),\n",
      "          padding=((1, 1), (1, 1)),\n",
      "          dilation=(1, 1),\n",
      "          groups=1,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        BatchNorm(\n",
      "          weight=f32[512],\n",
      "          bias=f32[512],\n",
      "          first_time_index=StateIndex(inference=False),\n",
      "          state_index=StateIndex(inference=False),\n",
      "          axis_name='batch',\n",
      "          inference=False,\n",
      "          input_size=512,\n",
      "          eps=1e-05,\n",
      "          channelwise_affine=True,\n",
      "          momentum=0.99\n",
      "        )\n",
      "      ],\n",
      "      skip=[]\n",
      "    ),\n",
      "    AvgPool2d(\n",
      "      init=0,\n",
      "      operation=<function add>,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    <wrapped function ravel>,\n",
      "    Linear(\n",
      "      weight=f32[10,512],\n",
      "      bias=f32[10],\n",
      "      in_features=512,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function log_softmax>\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Int, Float\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def loss(\n",
    "    model: ResNet18, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_y = jax.vmap(model, axis_name=\"batch\")(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the log-softmax'd predictions.\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example loss\n",
    "loss_value = loss(model, dummy_x, dummy_y)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "# Example inference\n",
    "output = jax.vmap(model,axis_name=\"batch\")(dummy_x)\n",
    "print(output.shape)  # batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.9789512, -2.3950043, -2.6886563, -1.8656045, -2.5833943,\n",
       "        -2.6215875, -1.9210961, -1.9728549, -2.5671916, -2.0814779],\n",
       "       [-3.008628 , -1.9291335, -3.0460534, -2.3002076, -2.5005283,\n",
       "        -2.6957893, -1.9287262, -1.9577394, -2.4861603, -1.9782414],\n",
       "       [-3.0528016, -2.137925 , -2.7659683, -2.175425 , -2.5703459,\n",
       "        -2.5074096, -1.970276 , -1.9773171, -2.6465983, -1.8861097],\n",
       "       [-3.0155644, -2.2839663, -2.5423312, -1.9006605, -2.805281 ,\n",
       "        -2.6580024, -1.9196169, -1.9611559, -2.7647502, -1.9659599],\n",
       "       [-2.8039777, -2.1381135, -3.162402 , -2.4216886, -2.884741 ,\n",
       "        -2.4300232, -1.8191922, -1.98104  , -2.613413 , -1.7533762],\n",
       "       [-4.1119623, -2.4638362, -3.9997034, -2.830008 , -3.0697064,\n",
       "        -2.1560962, -1.780485 , -1.8142817, -1.8747239, -1.7483344],\n",
       "       [-3.367105 , -2.0736187, -3.204104 , -2.4452202, -2.9308858,\n",
       "        -2.5664139, -1.8493356, -1.9590516, -2.1562192, -1.7824687],\n",
       "       [-3.200592 , -2.3197198, -2.788048 , -2.2934484, -3.0242262,\n",
       "        -2.61299  , -1.668727 , -2.1050546, -2.5432324, -1.6731582],\n",
       "       [-3.519612 , -1.9056604, -3.4082751, -2.5808895, -2.572544 ,\n",
       "        -2.3458269, -1.7342101, -2.1479917, -2.319182 , -1.9028777],\n",
       "       [-3.533778 , -2.3135319, -3.4671102, -2.4638205, -2.8346653,\n",
       "        -2.6277542, -1.6468198, -1.8941541, -2.1529534, -1.7996824],\n",
       "       [-3.1638193, -2.140214 , -2.816033 , -2.3121014, -2.8799028,\n",
       "        -2.5672445, -1.8580283, -1.9077537, -2.6164117, -1.766877 ],\n",
       "       [-3.4237094, -1.8114959, -3.5326982, -2.6522505, -2.9755135,\n",
       "        -2.3356528, -1.6884454, -2.517849 , -2.0116904, -1.8495337],\n",
       "       [-2.8839242, -2.362329 , -2.5339544, -1.9673246, -2.7273264,\n",
       "        -2.5702171, -1.861746 , -2.0796232, -2.722988 , -1.946731 ],\n",
       "       [-2.7615013, -2.2493553, -2.9783635, -1.9769605, -2.9817796,\n",
       "        -2.4964032, -1.9133197, -1.9546113, -2.6680844, -1.8951683],\n",
       "       [-2.890327 , -2.0556111, -2.8901708, -2.3752635, -2.593501 ,\n",
       "        -2.3986502, -1.8362238, -2.0856972, -2.5075443, -1.9865673],\n",
       "       [-3.1361203, -2.2125003, -3.0094013, -2.1550736, -2.7253256,\n",
       "        -2.4470696, -1.8970037, -2.0863612, -2.432533 , -1.7845945],\n",
       "       [-3.2024186, -1.7341521, -2.8337553, -2.1136193, -2.9467204,\n",
       "        -2.8030834, -1.9635576, -2.1437035, -2.3655956, -1.976243 ],\n",
       "       [-3.736813 , -2.0920484, -3.2952433, -2.4041762, -2.662493 ,\n",
       "        -2.2483404, -1.8374672, -1.8181175, -2.670987 , -1.8372575],\n",
       "       [-2.8548663, -2.3923466, -2.6166644, -1.918779 , -2.7097726,\n",
       "        -2.4007936, -2.0300992, -1.9529676, -2.7070556, -2.0092936],\n",
       "       [-3.0211473, -1.8537096, -3.066136 , -2.1481643, -3.0268154,\n",
       "        -2.6981535, -1.930179 , -2.0153422, -2.5533912, -1.8370484],\n",
       "       [-3.0832427, -2.3168356, -2.6031473, -1.8905044, -2.62564  ,\n",
       "        -2.3743427, -1.9230223, -2.0424643, -2.8274934, -2.0397172],\n",
       "       [-2.9883535, -2.0561101, -2.9661648, -2.0088005, -2.8484778,\n",
       "        -2.6411557, -1.9001707, -2.0540993, -2.6214898, -1.8555183],\n",
       "       [-3.4266677, -1.8324047, -3.604397 , -2.4032006, -3.1030214,\n",
       "        -2.54699  , -1.5514846, -2.2706304, -2.5530877, -1.7510405],\n",
       "       [-3.3302546, -1.974395 , -3.0751615, -2.1640332, -2.8575673,\n",
       "        -2.2988126, -1.7243563, -2.087687 , -2.286794 , -2.2760565],\n",
       "       [-3.48921  , -2.1531112, -3.5726924, -2.6593795, -2.943855 ,\n",
       "        -2.1224332, -1.8150048, -2.1305897, -2.0362186, -1.7675374],\n",
       "       [-3.0561752, -2.1309555, -2.8791013, -1.9864905, -2.7548814,\n",
       "        -2.4366884, -1.8947632, -2.046388 , -2.4507644, -2.087484 ],\n",
       "       [-2.9321973, -2.1085334, -2.8679848, -2.319765 , -2.87247  ,\n",
       "        -2.4479   , -1.6583691, -2.102027 , -2.4081774, -2.0832615],\n",
       "       [-3.0025363, -2.2165508, -2.8386502, -2.1273386, -2.5852036,\n",
       "        -2.6112556, -2.0772321, -1.7767336, -2.441423 , -2.0146296],\n",
       "       [-3.2008414, -2.3066578, -2.8872545, -2.2293124, -2.5523105,\n",
       "        -2.7825484, -1.6054108, -2.041603 , -2.563188 , -1.9046222],\n",
       "       [-3.14428  , -2.0522704, -3.031381 , -2.3104486, -2.8323638,\n",
       "        -2.3561368, -1.681359 , -2.1849384, -2.4193563, -1.967902 ],\n",
       "       [-3.1819253, -2.1991124, -2.9661698, -2.4388924, -2.9013624,\n",
       "        -2.6878846, -1.7794611, -2.0509589, -2.3661957, -1.6363748],\n",
       "       [-3.313786 , -2.0457702, -3.220595 , -2.03551  , -2.8328595,\n",
       "        -2.6686854, -1.6951131, -2.0826044, -2.432662 , -1.9688178],\n",
       "       [-3.0153742, -2.0565147, -2.7966576, -2.1413805, -2.7751465,\n",
       "        -2.7123632, -1.8684038, -1.9406685, -2.6747732, -1.9041648],\n",
       "       [-2.9511025, -2.3200445, -2.7157946, -1.9559412, -2.613794 ,\n",
       "        -2.3758838, -1.9689685, -2.2069063, -2.6056435, -1.8826933],\n",
       "       [-3.0845609, -2.2494   , -2.6361148, -2.1411746, -2.529915 ,\n",
       "        -2.3591428, -2.0109682, -2.1259744, -2.769311 , -1.7744082],\n",
       "       [-3.2555962, -2.3087354, -3.204986 , -2.4538577, -2.6572828,\n",
       "        -2.5702124, -1.5505604, -1.9770421, -2.580421 , -1.8169098],\n",
       "       [-2.8182065, -2.2822096, -2.9885259, -1.9996705, -2.7864823,\n",
       "        -2.4003978, -1.903468 , -2.0015523, -2.600056 , -1.9538879],\n",
       "       [-2.8914866, -2.3124247, -2.7775526, -1.8433902, -2.5478382,\n",
       "        -2.5447342, -1.9942284, -2.0565567, -2.6433163, -2.0164046],\n",
       "       [-3.0615923, -2.2311177, -2.557145 , -2.3104205, -2.432771 ,\n",
       "        -2.5958426, -1.8097217, -2.052386 , -2.7840452, -1.8790097],\n",
       "       [-3.1768131, -2.1554952, -3.0174522, -2.242632 , -2.855348 ,\n",
       "        -2.7765417, -1.7288983, -2.0457876, -2.4652252, -1.73844  ],\n",
       "       [-2.998847 , -2.2342882, -2.7231295, -2.1354103, -2.8143165,\n",
       "        -2.5393124, -1.8318163, -2.1490588, -2.4629712, -1.8422651],\n",
       "       [-3.0506623, -2.2865734, -2.8070402, -1.9640485, -2.5794535,\n",
       "        -2.4576948, -1.9254845, -1.9819381, -2.6484504, -2.0056689],\n",
       "       [-3.1012888, -2.0265234, -3.115881 , -2.534701 , -2.9222107,\n",
       "        -2.5447934, -1.8149594, -1.8663442, -2.4056587, -1.835408 ],\n",
       "       [-3.2765136, -1.8106518, -3.2967052, -2.3600137, -2.8388786,\n",
       "        -2.6351404, -1.921129 , -2.1334321, -2.383224 , -1.7151065],\n",
       "       [-3.2292044, -2.117993 , -3.110499 , -2.4154983, -2.9874487,\n",
       "        -2.4529033, -1.7853284, -1.977143 , -2.4198804, -1.7450349],\n",
       "       [-3.111061 , -2.099637 , -2.626798 , -2.2983346, -2.8382635,\n",
       "        -2.5828662, -1.8202455, -2.1386008, -2.6997313, -1.7198311],\n",
       "       [-3.0134747, -1.9768977, -2.8317816, -2.3379729, -3.0137548,\n",
       "        -2.7797005, -1.7353094, -2.0075028, -2.6281729, -1.8145788],\n",
       "       [-2.834627 , -2.4111228, -2.8376367, -2.1078928, -2.7691784,\n",
       "        -2.4158082, -1.9066745, -1.9300777, -2.5028853, -1.9384996],\n",
       "       [-3.1208763, -2.0969408, -2.674929 , -1.9560685, -2.6607637,\n",
       "        -2.443478 , -1.8855401, -2.0473425, -3.0710745, -1.9748948],\n",
       "       [-3.2222407, -2.050657 , -2.8896332, -2.3922179, -2.6960788,\n",
       "        -2.6080847, -1.8054776, -1.9065435, -2.7639432, -1.7880627],\n",
       "       [-2.818694 , -2.24902  , -2.7032733, -1.931759 , -2.7657924,\n",
       "        -2.5310245, -1.8495836, -1.9615697, -2.8138764, -2.0994916],\n",
       "       [-3.0849833, -2.340228 , -2.8954072, -2.0420704, -2.668516 ,\n",
       "        -2.489239 , -1.8993721, -1.9921567, -2.6136565, -1.8246796],\n",
       "       [-3.225173 , -2.009701 , -3.2087276, -2.530438 , -2.9835048,\n",
       "        -2.1784072, -1.8316839, -2.1027236, -2.2889168, -1.8406761],\n",
       "       [-3.4170866, -2.066582 , -3.310023 , -2.4350533, -3.1574464,\n",
       "        -2.4482427, -1.8497453, -2.2906513, -1.9227715, -1.6993225],\n",
       "       [-3.2298334, -2.3110902, -2.7688444, -2.3375714, -2.6461856,\n",
       "        -2.6556506, -1.6379354, -2.0209312, -2.777483 , -1.7619817],\n",
       "       [-2.8980384, -2.119053 , -2.72549  , -2.0364017, -2.6651149,\n",
       "        -2.7029614, -1.8701085, -2.0458095, -2.7534103, -1.9308764],\n",
       "       [-3.6531916, -1.7725928, -3.6434827, -2.3010945, -3.041892 ,\n",
       "        -2.634488 , -1.6221362, -2.0560632, -2.5306559, -1.875036 ],\n",
       "       [-3.1089191, -2.2345881, -2.9079673, -2.0587852, -2.8895967,\n",
       "        -2.7924035, -1.7915847, -1.979332 , -2.4609165, -1.8386446],\n",
       "       [-3.3044896, -1.930978 , -3.04624  , -2.3639238, -2.9537141,\n",
       "        -2.6351924, -1.7570702, -2.0293992, -2.5471115, -1.768636 ],\n",
       "       [-3.0686667, -2.314723 , -2.5996964, -1.956044 , -2.9723692,\n",
       "        -2.453029 , -1.8744915, -2.11254  , -2.6690426, -1.8444395],\n",
       "       [-2.941927 , -2.1683376, -2.771878 , -2.1108322, -2.701982 ,\n",
       "        -2.5763636, -1.772831 , -2.0154364, -2.76543  , -1.9661   ],\n",
       "       [-3.8836343, -1.8453043, -3.8647995, -2.6956496, -2.9830341,\n",
       "        -2.5038912, -1.6387658, -1.9080442, -2.4492304, -1.7624371],\n",
       "       [-3.4157586, -1.9692699, -3.0342417, -2.5101469, -2.787942 ,\n",
       "        -2.37375  , -1.7774327, -2.0840077, -2.5835059, -1.7456425],\n",
       "       [-2.9352682, -2.1561284, -2.940951 , -2.2164946, -2.647503 ,\n",
       "        -2.26233  , -1.8529818, -2.0804725, -2.487733 , -2.0430605]],      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: ResNet18, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = jax.vmap(model,axis_name=\"batch\")(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def evaluate(model: ResNet18, testloader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2.3932571, dtype=float32), Array(0.09136146, dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: ResNet18,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    ") -> ResNet18:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: ResNet18,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 3 32 32\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, testloader)\n",
    "            print(\n",
    "                f\"{step=}, train_loss={train_loss.item()}, \"\n",
    "                f\"test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\"\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, train_loss=2.3915891647338867, test_loss=2.5397748947143555, test_accuracy=0.0895700678229332\n",
      "step=30, train_loss=1.985224962234497, test_loss=2.014582395553589, test_accuracy=0.2395501583814621\n",
      "step=60, train_loss=2.0089426040649414, test_loss=1.833449125289917, test_accuracy=0.3064291477203369\n",
      "step=90, train_loss=1.9649834632873535, test_loss=1.7725344896316528, test_accuracy=0.3427547812461853\n",
      "step=120, train_loss=1.8716309070587158, test_loss=1.7114099264144897, test_accuracy=0.3677348792552948\n",
      "step=150, train_loss=1.7450339794158936, test_loss=1.6816104650497437, test_accuracy=0.378085196018219\n",
      "step=180, train_loss=1.627855658531189, test_loss=1.6423696279525757, test_accuracy=0.39649683237075806\n",
      "step=210, train_loss=1.6442811489105225, test_loss=1.6072242259979248, test_accuracy=0.3958996832370758\n",
      "step=240, train_loss=1.6049822568893433, test_loss=1.573119044303894, test_accuracy=0.4198845624923706\n",
      "step=270, train_loss=1.5509659051895142, test_loss=1.5599132776260376, test_accuracy=0.41918790340423584\n",
      "step=300, train_loss=1.5777379274368286, test_loss=1.5400314331054688, test_accuracy=0.43590766191482544\n",
      "step=330, train_loss=1.6309890747070312, test_loss=1.535754680633545, test_accuracy=0.4321258068084717\n",
      "step=360, train_loss=1.5306487083435059, test_loss=1.5157427787780762, test_accuracy=0.4406847059726715\n",
      "step=390, train_loss=1.5377042293548584, test_loss=1.483795404434204, test_accuracy=0.44526275992393494\n",
      "step=420, train_loss=1.5101499557495117, test_loss=1.4684723615646362, test_accuracy=0.4689490497112274\n",
      "step=450, train_loss=1.7004441022872925, test_loss=1.4889402389526367, test_accuracy=0.4616839289665222\n",
      "step=480, train_loss=1.5823338031768799, test_loss=1.468345284461975, test_accuracy=0.4622810482978821\n",
      "step=510, train_loss=1.4491674900054932, test_loss=1.45572829246521, test_accuracy=0.4622810482978821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train(model, trn, tst, optim, STEPS, PRINT_EVERY)\n",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, testloader, optim, steps, print_every)\u001b[0m\n\u001b[1;32m     38\u001b[0m     model, opt_state, train_loss \u001b[39m=\u001b[39m make_step(model, opt_state, x, y)\n\u001b[1;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m print_every) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (step \u001b[39m==\u001b[39m steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m         test_loss, test_accuracy \u001b[39m=\u001b[39m evaluate(model, testloader)\n\u001b[1;32m     41\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m     42\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m=}\u001b[39;00m\u001b[39m, train_loss=\u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_loss=\u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m, test_accuracy=\u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, testloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# and both have JIT wrappers, so this is fast.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss(model, x, y)\n\u001b[0;32m---> 14\u001b[0m     avg_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m compute_accuracy(model, x, y)\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m avg_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(testloader), avg_acc \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(testloader)\n",
      "File \u001b[0;32m~/NF/iresnetenv/lib/python3.9/site-packages/equinox/_jit.py:75\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fun_wrapper(\u001b[39mFalse\u001b[39;00m, args, kwargs)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fun_wrapper(\u001b[39mFalse\u001b[39;49;00m, args, kwargs)\n",
      "File \u001b[0;32m~/NF/iresnetenv/lib/python3.9/site-packages/equinox/_jit.py:64\u001b[0m, in \u001b[0;36m_JitWrapper._fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached\u001b[39m.\u001b[39mlower(dynamic, static)\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     dynamic_out, static_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached(dynamic, static)\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m combine(dynamic_out, static_out\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/NF/iresnetenv/lib/python3.9/site-packages/equinox/_module.py:296\u001b[0m, in \u001b[0;36mModule._tree_unflatten\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    289\u001b[0m             static_field_values\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(dynamic_field_values), (\n\u001b[1;32m    291\u001b[0m         \u001b[39mtuple\u001b[39m(dynamic_field_names),\n\u001b[1;32m    292\u001b[0m         \u001b[39mtuple\u001b[39m(static_field_names),\n\u001b[1;32m    293\u001b[0m         \u001b[39mtuple\u001b[39m(static_field_values),\n\u001b[1;32m    294\u001b[0m     )\n\u001b[0;32m--> 296\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tree_unflatten\u001b[39m(\u001b[39mcls\u001b[39m, aux, dynamic_field_values):\n\u001b[1;32m    298\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    299\u001b[0m     dynamic_field_names, static_field_names, static_field_values \u001b[39m=\u001b[39m aux\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(model, trn, tst, optim, STEPS, PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
